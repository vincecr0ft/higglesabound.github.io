<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Higgles Abound Blog</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2018-11-11T00:26:00+01:00</updated><entry><title>Adventures in Torch and Deep Art</title><link href="/TorchDeepArt.html" rel="alternate"></link><published>2018-11-11T00:26:00+01:00</published><updated>2018-11-11T00:26:00+01:00</updated><author><name>Vincent Croft</name></author><id>tag:None,2018-11-11:/TorchDeepArt.html</id><summary type="html">&lt;p&gt;I setup torch with OpenCL to use the GPU on my laptop to make pretty pictures. Then I scaled the problem up on Google Cloed Conmpute Engine.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Adventures in Torch   and Deep Art&lt;/h1&gt;
&lt;p&gt;Recently I made a hanging canvas made for my Mother in law based on the idea of deep style transfer.&lt;/p&gt;
&lt;p&gt;After several attempts with various implemetations of the code I settled on torch implementation of the paper &lt;a href="http://arxiv.org/abs/1508.06576"&gt;A Neural Algorithm of Artistic Style&lt;/a&gt; by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge as implemented by &lt;a href="https://github.com/jcjohnson"&gt;Justin Johnson&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;For the last 2 years I've been lugging around my 15inch macbook pro - 4 times the weight and twice the price of a mac that would accomodate most of my needs... but this one has a dedicated GPU! ...but... that GPU is an intel Radeon Pro 450 and therefore not CUDA compatible and therefore not useful for machine learning.&lt;/p&gt;
&lt;p&gt;Torch is a lua front end to the TH tensor processing library and a predecessor to pytorch. Unlike pytorch however, Torch supports OpenCL execution and thereby makes my computer useful again! This allowed me to directly observe what all this fuss about GPU programming is about... about a 50 times speed upgrade! &lt;/p&gt;
&lt;p&gt;But even this new found magical power had its limitations. The image being manipulatied cannot be larger than 1024x1024 or else the GPU runs out of memory.&lt;/p&gt;
&lt;p&gt;Enter the power of the cloud. Setting up torch on GCP was not as straight forward as id like but given that I did it 16 times in an afternoon I think it cant have been that bad.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/vincecr0ft/vincecr0ft.github.io/source/content/images/montbug.png" height="400px"&gt;&lt;/p&gt;
&lt;p&gt;Want to give it a try?&lt;/p&gt;
&lt;h2&gt;Torch + CUDA on Google Cloud Platform&lt;/h2&gt;
&lt;p&gt;I launched the session As I was taught during my tesor flow certification. I navigatied to &lt;a href="https://console.cloud.google.com"&gt;the Google Cloud Console&lt;/a&gt; and started a ubuntu instance. Living in europe I found the only site to reliably provide me with GPU instances was the Belgian host at &lt;code&gt;europe-west1-b&lt;/code&gt; first I ran out of swap space when trying to install &lt;a href="https://developer.nvidia.com/cudnn"&gt;&lt;code&gt;cudnn&lt;/code&gt;&lt;/a&gt; and had to boot up a new instance. I had numerous problems when trying to use a dedicated deeplearning image (pytorch or tensorflow) so I stuck to plain ubuntu 16.04 with a couple of CPUs and at least 4 GPUs (I also tried with just one but ran into the same memory issues as when running locally):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ gcloud compute --project &lt;span class="s1"&gt;&amp;#39;your-project-name&amp;#39;&lt;/span&gt; ssh --zone &lt;span class="s1"&gt;&amp;#39;your-zone&amp;#39;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;your-instance-name&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;once in the instance i followed steps to install CUDA:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ curl -O http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_8.0.61-1_amd64.deb
$ sudo dpkg -i cuda-repo-ubuntu1604_8.0.61-1_amd64.deb
$ sudo apt-get update
$ rm cuda-repo-ubuntu1604_8.0.61-1_amd64.deb
$ sudo apt-get install cuda-8-0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then installed torch7 - hopefully picking up the CUDA install:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git clone https://github.com/torch/distro.git ~/torch --recursive

$ &lt;span class="nb"&gt;cd&lt;/span&gt; ~/torch&lt;span class="p"&gt;;&lt;/span&gt; bash install-deps&lt;span class="p"&gt;;&lt;/span&gt;

$ ./install.sh
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;this can take a while - especially if your CUDA istallation succeeded. I did have some minor issues with OpenBlas but nothing a quck google and some Ubuntu-fu can't fix.&lt;/p&gt;
&lt;p&gt;Finally (before checking out the main repo) we need loadcaffe to load a pretrained model from the Caffe Model Zoo:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo apt-get install libprotobuf-dev protobuf-compiler
$ sudo ~/torch/install/bin/luarocks install loadcaffe
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now its time for the actual code!&lt;/p&gt;
&lt;h2&gt;Neural Style&lt;/h2&gt;
&lt;p&gt;the readme for this code is great and the examples folder is great. Just check it out and run it!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git clone https://github.com/jcjohnson/neural-style.git
$ &lt;span class="nb"&gt;cd&lt;/span&gt; neural_style&lt;span class="p"&gt;;&lt;/span&gt; sh models/download_models.sh&lt;span class="p"&gt;;&lt;/span&gt;
$ th neural_style.lua -style_image &amp;lt;image.jpg&amp;gt; -content_image &amp;lt;image.jpg&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;but wait! that last step needs local images! how do I get them onto the GCP instance? &lt;a href="https://cloud.google.com/sdk/"&gt;Cloud SDK&lt;/a&gt;. &lt;em&gt;LOCALLY&lt;/em&gt; download the code for your system and run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ./google-cloud-sdk/install.sh
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Restart your terminal&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ gcloud init
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and log in! Now we can do an scp:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ gcloud compute scp /folder-to-your-file-locally/image.jpg instance-name:/tmp/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;whist there are lots of fun tutorials out there, and playing around with all the possible inputs and styles is crazy addictive, I found the &lt;a href="https://github.com/jcjohnson/neural-style/blob/master/examples/multigpu_scripts/starry_stanford.sh"&gt;starry_stanford.sh&lt;/a&gt; script gave the best results but needs some mighty powerful GPUs or to step down to CPU only mode in the end.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/vincecr0ft/vincecr0ft.github.io/source/content/images/montbug.png" height="400px"&gt;&lt;/p&gt;
&lt;p&gt;Oh and I think Im still on the GCP free/trial tier so it still says that I have 0 estimated costs.&lt;/p&gt;</content><category term="GPU"></category><category term="Torch"></category><category term="Photography"></category><category term="Deep Learning"></category></entry><entry><title>A Tauists Hunt For Higgles (part one)</title><link href="/ATauistsHuntForHiggles.html" rel="alternate"></link><published>2018-09-18T15:16:00+02:00</published><updated>2018-09-18T15:16:00+02:00</updated><author><name>Vince "the Higgler" Croft</name></author><id>tag:None,2018-09-18:/ATauistsHuntForHiggles.html</id><summary type="html">&lt;p&gt;For my PhD defence I was asked to explain my research at a level that my mother would understand... the result was cartoon pigs.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;A Tauists Hunt For Higgles (part one)&lt;/h1&gt;
&lt;p&gt;We begin with an explanation of nomenclature; a description of an evolution of grammar. When new terms become established and their names become more common place, the descriptive additional words start to become implied, eventually to become superfluous. &lt;/p&gt;
&lt;p&gt;&lt;img alt="well hello" src="images/wave.gif" title="Hello"&gt;&lt;/p&gt;
&lt;p&gt;This is Higgle Wiggle. Higgle Wiggle is not a Higgs boson, he is a picture of a Higgs boson. A subtle difference but an important one. A new bosonic resonance was discovered in 2012 that fits the description of the Higgs boson. In the years since its discovery, Higgs hunters and Higgs theorists have begun to drop the word boson when referring to the Higgs. But one Higgs, a measurement makes not. So it progresses: one Higgs, two Higgles and a Higgledy Piggledy of Higgles.&lt;/p&gt;
&lt;p&gt;&lt;img alt="heave ho!" src="images/push.gif"&gt;&lt;/p&gt;
&lt;p&gt;But how does this picture differ from the 'real' thing? Well the picture is a manifestation of our understanding of reality designed to relate the characteristics of the thing into an single entity that can be described be someone who has never met Higgle Wiggle before. The standard model of particle physics is just that, a model. A picture that relates the characteristics of what we see in our detector to the underlying mathematics of the universe. &lt;/p&gt;
&lt;p&gt;Ok but how do we get from this picture to a better understanding of the Higgs? Logic. So let's apply some!&lt;/p&gt;
&lt;p&gt;&lt;img alt="a wild theory has appeared" src="images/zoom.gif"&gt;&lt;/p&gt;
&lt;p&gt;Higgle Wiggle lives in the smallest space that we can see. Imagine a creature so small that the lines that form your fingerprints form the towering city blocks of the sprawling metropolis it calls home. Now imagine this creature looks down at the protein strands of its ganglia and pictures the city scape formed by a single nucleus in a single atom. That's where Higgle Wiggle lives.&lt;/p&gt;
&lt;p&gt;&lt;img alt="huh?" src="images/exchange.gif"&gt;&lt;/p&gt;
&lt;p&gt;Now let's say Higgle Wiggle wants to communicate with a tau neutrino: he can't shout; at these small scales theres no air for the sound waves to vibrate through, nor do our point source particles have mouths to speak or ears to hear. Our particles can't wave (even if they had arms) since light at this scale acts as a particle in its own right. But even worse, some particles only speak certain languages. e.g. only particles with electrical charge can interact with photons (light particles). The neutrino is neutral. The neutrino only interacts weakly and even then it also restricts itself to making sure that it doesn't interact unless another tauish particle is also present. So there it is. In order for Higgle Wiggle to influence or interact with a tau neutrino it must rip off a part of itself in the form of a tau lepton and throw it at the neutrino.&lt;/p&gt;
&lt;p&gt;&lt;img alt="huh?" src="images/exchange2.gif"&gt;&lt;/p&gt;
&lt;p&gt;But the standard model is built from symmetries and conservations. When the negatively charged tauish lepton leaves the Higgs what remains must be positive and also tauish. What remains is an anti-tau lepton. But by the same token the neutrino is also changed, leaving behind only a weekly interacting particle with the electrical charge of the tau lepton.&lt;/p&gt;
&lt;p&gt;&lt;img alt="huh?" src="images/exchange6.gif"&gt; &lt;/p&gt;</content><category term="Higgs mechanism"></category><category term="outreach"></category><category term="animation"></category><category term="theory"></category></entry><entry><title>High Energy Statistics</title><link href="/high-energy-statistics.html" rel="alternate"></link><published>2018-06-16T14:04:00+02:00</published><updated>2018-06-16T14:04:00+02:00</updated><author><name>Vincent Alexander Croft</name></author><id>tag:None,2018-06-16:/high-energy-statistics.html</id><summary type="html">&lt;p&gt;Why sch small particles need such big data sets.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;High Energy Statistics&lt;/h1&gt;
&lt;p&gt;Before I begin apparently this word has become a piece of high energy physics jargon. However its usage has become so ingrained in everything I do. High statistics = lots of events, low statistics = not many data points. It has got to the point where I am not even sure that this is not common usage of the term. But there it is. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://royalsociety.org/science-events-and-lectures/2017/11/how-do-plants-grow/"&gt;&lt;img alt="How plants grow by Dr Rucha Karnik" src="/images/how-plants-grow.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Several months ago I listened to a RI discussion on some issue in botany as a podcast whilst out running. A member of the audience asked an uneducated question to which I found myself rattling off a standard particle physics response. The subject matter was of a statistical nature. In science &lt;span class="math"&gt;\(X\)&lt;/span&gt; rarely happens because of &lt;span class="math"&gt;\(Y\)&lt;/span&gt;, instead we have evidence for &lt;span class="math"&gt;\(X\)&lt;/span&gt; in some sample &lt;span class="math"&gt;\(Y\)&lt;/span&gt;. It prompted a long think of the similarities between particle physics and botany. &lt;/p&gt;
&lt;h2&gt;The First Modern Statistician.&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Ronald_Fisher"&gt;&lt;img alt="R.A.Fisher" src="/images/Fisher.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;After a brief stint teaching, Ronald Aylmer Fisher turned down Karl Pearson, the worlds foremost expert on statistical theory, and a position at University College London, to instead move to work for at the Rothamsted Experimental Station. Here he could look at a wealth of agricultural data that had been accumulated over 80 years. It was during his time here that he developed what became the foundations of modern statistical theory, in the invention of the analysis of variance (ANOVA), error functions of statistics and popularised modern usage of Maximum Likelihood Methods and p-values.&lt;/p&gt;
&lt;h3&gt;But why plants?&lt;/h3&gt;
&lt;p&gt;Randomness, is a part of nature. Randomness is just as crucial to evolutionary biology as sunlight or nutrients. Thus on trying to analyse why crops behaved this way or that, Fisher had to master randomness itself in order to arrive at the truth. &lt;/p&gt;
&lt;p&gt;The wealth of data at Rothamsted allowed Fisher to forever change the way we assess the outcome of experiments. When randomness is involved high statistics is the only way to extract the truth&lt;/p&gt;
&lt;h3&gt;Scaling up to Particles&lt;/h3&gt;
&lt;p&gt;Particle physics has randomness built in. Randomness that Einstein famously rejected saying 'God does not play dice!' quantum weirdness is weird because probability plays a leading roll in in describing how particles live and die. Each of the primary experiments at the LHC produces more data the entire 80 year Rothamstead archive every single second. As such every analyst is trained to see the quantity of data received as statistics. The Higgs boson isn't a lump of rock that we poke with sticks and write down how it wiggles. The plots and distributions are literally manifestations of how the Higgs looks and behaves.&lt;br&gt;
&lt;img alt="It's a Higgle!" src="/images/HiggsWW.gif"&gt;&lt;/p&gt;
&lt;h2&gt;Bigger Stats = Bigger Data&lt;/h2&gt;
&lt;p&gt;Hence the need for the world wide web and now the grid. Why physics departments around the world are investing billions into high energy physics oriented data centres. Just like the randomness in biology that sparked a revolution in evolutionary genetics, quantum weirdness in particle physics is pushing for faster analyses, of denser information, of quantities of data never before seen on experimental scales. Our models can involve hundreds or even tens of thousands of parameters, and the billions of measurements split over several measurements and statistics is at the heart of it all. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://youtu.be/dMwP7nEuDGA"&gt;&lt;img alt="Cern Data Centre" src="/images/datacentre.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="statistics"></category><category term="computing"></category><category term="history"></category></entry></feed>